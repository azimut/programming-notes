#+STARTUP: latexpreview
#+OPTIONS: tex:t

- Words
  - Ground Truth = real life observations
  - Model = our prediction f(x)
  - Loss = prediction's error rate with our Ground Truth
  - Mean Squared Error = a way to calculate Loss
  - Bias = a shift
  - Hyperparameters = parameters to our train() fn
  - Gradient Descent = a way to find a minimum to our loss fn
    - Gradient = curve of the loss
    - Partial Derivatives = a way to calculate gradient descend with multiple variables

- Types:
  - ~Reinforment Learning~: we want to optimize something
    - we only know in *relative* terms how good/bad a solution is
    - eg: an AI that "learns" to play a game by playing
  - ~Supervised Learning~: we provide *labeled* data.
    - By =function approximation= f(x)
    - Divided in phases:
      1. Training: finding f(x)
      2. Prediction: using f(x)
    - Linear Regression:
      #+CAPTION: w=weight b=bias
      #+begin_src
       ŷ = x * w + b
      #+end_src
  - ~Unsupervised Learning~: from *unlabeled* data
    - it learns about the relationships between the inputs provided.
    - Used for _clustering_ into groups.
    - Used to improve the quality of data.
    - Used for compress data.

- article: https://karpathy.github.io/2019/04/25/recipe/
- course: pragmatic data science https://www.youtube.com/playlist?list=PLjxbCynJ0Gd8k75-zdMcqcEbH90jfk9eg
- article: llm agent https://tadeodonegana.com/posts/building-agents-my-notes/
- DataSets
  - https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research
  - https://en.wikipedia.org/wiki/Iris_flower_data_set
  - https://en.wikipedia.org/wiki/MNIST_database
  - https://en.wikipedia.org/wiki/ImageNet
  - https://github.com/several27/FakeNewsCorpus

- deep learning https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8
- https://github.com/AudioLLMs/AudioLLM
- Course: Introduction to ML https://www.youtube.com/playlist?list=PLuh62Q4Sv7BUhFs1KpeVctFmOYpTTxj33
- Course: intro for probability for cs https://www.youtube.com/playlist?list=PLoROMvodv4rOpr_A7B9SriE_iZmkanvUg
- Course: zero to neural networks https://www.youtube.com/@datacorelinux/videos
- https://microsoft.github.io/generative-ai-for-beginners/#/
- books https://github.com/aridiosilva/AI_Books/
- https://uvadlc-notebooks.readthedocs.io/en/latest/index.html
- Course: Neural Networks https://karpathy.ai/zero-to-hero.html
  - Neural Networks: Zero to Hero  https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ
- Video:  How to Create a Neural Network (and Train it to Identify Doodles)
  https://www.youtube.com/watch?v=hfMk-kjRv4c
- MIT 6.S191 Introduction to Deep Learning https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI
- MIT Self Driving Lectures https://www.youtube.com/playlist?list=PLrAXtmErZgOeY0lkVCIVafdGFOTi45amq
- https://stackoverflow.blog/2022/04/21/the-robots-are-coming-for-the-boring-parts-of-your-job/?cb=1
- Standford Machine Learning Andrew NG https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&index=1
- Course: 2020 Cornell CS 5787: Applied Machine Learning.
  https://www.youtube.com/playlist?list=PL2UML_KCiC0UlY7iCQDSiGDMovaupqc83
- NN https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi
- MIT RES.LL-005 Mathematics of Big Data and Machine Learning, IAP 2020
  https://www.youtube.com/watch?v=t4K6lney7Zw
- Video: Information Thoery, Pattern Recognition, and Neural Networks
  https://www.youtube.com/playlist?list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6
- https://www.youtube.com/user/mathematicalmonk/videos
- Video Channel
  https://www.youtube.com/c/ShashankKalanithiData/videos
- Video: Conal Elliott: Can Tensor Programming Be Liberated from the Fortran Data Paradigm?
  https://www.youtube.com/watch?v=oaIMMclGuog
  https://github.com/conal/talk-2021-can-tensor-programming-be-liberated
- Deep Learning Book Lectures https://www.youtube.com/c/AlenaKruchkova/videos
- MIT http://introtodeeplearning.com/
  https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI
- (Welch Labs) Neural Networks Demystified https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU
- http://karpathy.github.io/neuralnets/
- Andrew Ng https://www.coursera.org/learn/machine-learning
- CS231n: Convolutional Neural Networks for Visual Recognition http://vision.stanford.edu/teaching/cs231n/
- https://twitter.com/cfiesler/status/1336317217034612737
  Algorithms of Oppresion
  The Age of Surveillance Capitalism
  Race After Technology
  Weapons of Math Destruction
  Automating Inequality
  Technically Wrong
  Ghost Work
  Design Justice
- https://ml4code.github.io/papers.html
- https://medium.com/@satnalikamayank12/on-automated-generation-of-commit-messages-from-code-differences-7ab205ae580
- Deeplearning - Udacity - https://www.youtube.com/playlist?list=PLAwxTw4SYaPn_OWPFT9ulXLuQrImzHfOV
- https://www.edx.org/course/artificial-intelligence-for-everyone?source=aw&awc=6798_1596893433_4b4c0888ce9c6d92a5a2ae929d88d9c7&utm_source=aw&utm_medium=affiliate_partner&utm_content=text-link&utm_term=301045_https%3A%2F%2Fwww.class-central.com%2F
- [Coursera] Neural Networks for Machine Learning — Geoffrey Hinton
  https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9
- Vincent Warmerdam: Winning with Simple, even Linear, Models | PyData London 2018
  https://www.youtube.com/watch?v=68ABAU_V8qI
- https://github.com/mrdbourke/machine-learning-roadmap
- https://github.com/visenger/awesome-mlops
- CS50's Introduction to Artificial Intelligence with Python 2020
  https://www.youtube.com/playlist?list=PLhQjrBD2T382Nz7z1AEXmioc27axa19Kv
- NARDOZ MARZO - Fairness en Machine Learning + Testing en desarrollo de software
  https://www.youtube.com/watch?v=rrwrornKhjM
- https://github.com/mitmath/18337
- Toward ethical, transparent and fair AI/ML:
  a critical reading list for engineers, designers, and policy makers
  https://github.com/rockita/criticalML
- https://www.youtube.com/playlist?list=pll8olhzgyoq7bkvburthesalr7bonzbxs
- channel https://www.youtube.com/@vlavrenko/playlists
  - neural networls and backpropagation https://www.youtube.com/watch?v=jzyz0eupybi&list=plbv09bd7ez_4bs9j3o8l_ztjqzon_3oqs

* channels
- will kwan https://www.youtube.com/c/willkwan/videos
- instituto de calculo secretaria https://www.youtube.com/channel/uczcbeanqeihofbrbdyx1nea
- https://www.youtube.com/c/sirajraval/videos
- https://www.youtube.com/c/yannickilcher/videos
- https://www.youtube.com/user/ministeriodeciencia/videos
- https://www.youtube.com/channel/ucfxnrdbm1yrv9j2mb8aiy4q
* video: practical deep learning for coders (2020)
https://www.youtube.com/playlist?list=plfyubjixbdtrl3fmb3gowhri8ieu6fhfm
** lesson 1 https://www.youtube.com/watch?v=_quexshfsa0
- neural networks have limited range of things it can do with 1 layer (by minsky research)
- but with more layers the problem is solved (also by minsky)
- people used just 2 layers which limited performance
- usinge more layers makes it "deep" learning

* video: 2011 - machine learning for the web - hilary mason
** 1 introduction
- classification problems:
  - clustering of categorical data
  - named entity disambiguation: separate different entities (similar)
- recommendation systems
- special data (uses domain knowledge)
  - geographic
  - timeseries
- approaches from:
  - having data
  - having a problem
  - have infrastructure
- methodology:
  1) obtain
  2) scrub
  3) explore
  4) model
  5) interpret
** 2 classifying web documents - the theory
- problems harder to classify (write logic)
- supervised learning
  - examples:
    - spam classification
    - language identification
    - face detection
- book: "data source handbook"
- nyt has <meta> tags with information about the article.

* book: 2019 | grokking deep learning                    | andrew w. trask
https://github.com/iamtrask/grokking-deep-learning
* book: 2021 | loving common lisp                        | mark watson
** backpropagation neural networks
- trained by  applying training inputs to the networks
- compare differences/errors between
  1) propagated values
  2) training data values
- we magnitude of these errors are used to adjust the weights in the network
- some problems while trying to find "good enough" weights
  1) (randomness) sometimes he accumulated error at a *local minimum* is too large, is best to restart the training
  2) (memory) if we have enough *memory* and with not enough data, we might just memorize the training data.
     memory=weights. start using a small network.
- the ~activation values~ of individual neurons are limited to the range [0,1].
  - sum of the activation values of neurons in the previous layer *times* the values of the connecting weights and then
    using *sigmoid* function to map the sums to desired range.
* book: 2021 | deep learning: a visual approach          | andrew glassner
  https://nostarch.com/deep-learning-visual-approach
  https://github.com/blueberrymusic/deep-learning-a-visual-approach (scikit-learn)
** introduction
- know, stats (to know how to describe the "patterns" in the data)
- know, ~bayes~ (to know the likelihood an algorithm is correct)
- know, it (information theory) to measure kinds of information
- do, machine learning classification to explore the data we have before dl
- know, ensambles of different ml systems instead of a big one, sometimes is better
- ~backpropagation~ (a way of training) and ~optizers~ (modifies the network numbers)
- ~convnet~ (convolution neural networks) made to handle spatial data, like images. like recognizing objects.
- ~autoencoders~ simplify datasets, or clean images (?
- ~recurrent neural networks~ for sequences (text or audio)
- ~attention and transformers~ to interpret and generate text
- ~reinforment learning~ ?
- ~generative adversarial networks~ to generate data
** part 1
** 1 an overview of machine learning
- our goal (with ml) is to discover *meaningful* information,
  where is up to us decide what's *meaninful*.
- ~expert systems~: we create rules from what the experts tells us. feature engineering.
- ~supervised learning~: we provide *labeled* data.
  when the system gets enough right answers for our needs we can say it is *trained*
- ~unsupervised learning~: it learns about the relationships between the inputs provided.
  used for clustering into groups.
  used to improve the quality of data.
  used for compress data.
- ~reinforment learning~ when we search to optimize (? something, but we don't know how.
  while we judge how good or bad the algorthim is in relative terms. ("probably good", "better than the last one")
  it can be always searching with new data, while using the "best" solution found.
- ~deep learning~ uses a series of steps or *layers* for computation
- neurons turn input value into a number.
  neurons stay the same, what can change is the input and weights
  initial weights are random.
  loop -> weights are adjusted carefully by a small ammount. and output is judged.
  neurons converge into looking for *features* although we never told him to.
** 2 essential statistics
* book: 2020 | programming ml from coding to dl          | paolo perrotta

** 1 how machine learning works

https://pragprog.com/titles/pplearn/programming-machine-learning/

#+begin_src sh
  $ pip3 install numpy==1.15.2
  $ pip3 install matplotlib==3.1.2
  $ pip3 install seaborn==0.9.0
#+end_src

[[https://news.stanford.edu/2017/11/15/algorithm-outperforms-radiologists-diagnosing-pneumonia/][example of a machine learning solution]]

** 2 your first learning program

*** example: pizzas per reservations

#+begin_src python
  x, y = np.loadtxt("pizza.txt", skiprows=1, unpack=true)
#+end_src

linear regression
#+begin_src python
  def predict(x, w): # our model
      return x * w
#+end_src

how measure loss our model?: mean squared error
avg of difference between prediction loss and ground truth, squared
#+begin_src python
  def loss(x, y, w):
      return np.average((predict(x,w) - y) ** 2)
#+end_src

iterate, trying to find a suitable *w*.
with a given *lr* (learning rate) step.
#+begin_src python
  def train(x, y, iterations, lr):
      w = 0 # arbitrary init value
      for i in range(iterations):
          current_loss = loss(x, y, w)
          print("iteration %4d => loss: %.6f" % (i, current_loss))
          if loss(x, y, w + lr) < current_loss:
              w += lr
          elif loss(x, y, w - lr) < current_loss:
              w -= lr
          else:
              return w

      raise exception("couldn't converge within %d iterations" % iterations)
#+end_src

*** example: adding a bias

#+caption: w=weight b=bias
#+begin_src src
  ŷ = x * w + b
#+end_src

#+begin_src python
  def predit(x, w, b):
      return x * w + b

  def loss(x, y, w, b):
      return np.average((predict(x,w,b) - y) ** 2)

  def train(x, y, iterations, lr):
      w = b = 0
      for i in range(iterations):
          current_loss = loss(x, y, w, b)
          if   loss(x,y,w+lr,b) < current_loss:
              w += lr
          elif loss(x,y,w-lr,b) < current_loss:
              w -= lr
          elif loss(x,y,w,b+lr) < current_loss:
              b += lr
          elif loss(x,y,w,b-lr) < current_loss:
              b -= lr
          else:
              return w, b

      raise exception("couldn't converge within %d iterations" % iterations)
#+end_src

*** example: plot code

#+begin_src python
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns

  sns.set()
  plt.axis([0,50,0,50])
  plt.xticks(fontsize=15)
  plt.yticks(fontsize=15)
  plt.xlabel("reservations", fontsize=30)
  plt.ylabel("pizza", fontsize=30)
  x, y = np.loadtxt("pizza.txt", skiprows=1, unpack=true)
  plt.plot(x,y,"bo")
  plt.show()
#+end_src

** 3 walking the gradient

- problems with our current train()
  1) doesn't scale well (cpu/time) when adding new hyperparameters
  2) is not precise since hyperparameters defined in lr terms

- observation:
  - if we plot our loss() function values with b=0
  - it would look like a uv curve

- 𝛿l/𝛿w - =gradient descent=
  - to measure the gradient
  - "the derivative of the loss with respect to the weight"
  - derivative on point is -0, if loss decr when w does it
  - derivative on point is +0, if loss incr when w does it
  - derivative on point is  0, if is minimum

*** A Sprinkle Of Math

#+CAPTION: mean squared error loss
$$L = \frac{1}{m} \sum_{i=1}^{m} (({wx_i}+{b}) - {y_i})^2$$

Where $${m}$$ is the number of examples.

#+CAPTION: derivative of L with respect to w
$$\frac{\partial{L}}{\partial{w}} = \frac{2}{m} \sum_{i=1}^{m} {x_i} (({wx_i}+{b}) - {y_i})$$

*** Downhill Riding

#+CAPTION: our new gradient function
#+begin_src python
  def gradient(X,Y,w):
      return 2 * np.average(X * (predict(X,w,0) - Y))
#+end_src

#+begin_src python
  def train(X,Y,iterations,lr):
      w = 0
      for i in range(iterations):
          print("Iteration: %4d => Loss: %.10f" %
                (i, loss(X,Y,w,0)))
          w -= gradient(X,Y,w) * lr # !
      return w
#+end_src
- no *ifs* needed
- we step in the opposite direction of gradient()
- we don't stop early
- good hyperparameters by trial&error
  - iterations=100
  - lr=0.001
  - w=1.8436928702

*** Escape from Flatland

- if we consider $${b}\neq0$$
- loss() becomes a 3D surface
- =Partial Derivatives=
  - a way to calculate gradient descend with multiple variables
  - first calculating the gradient of a lower dimension slice
  - then combining the slices to get the gradient of the surface

#+CAPTION: derivative pretending w is constant
$$\frac{\partial{L}}{\partial{b}} = \\
  \frac{2}{m} \sum_{i=1}^{m} (({wx_i}+{b}) - {y_i})$$

*** Putting Gradient Descent To The Test

#+begin_src python
  def gradient(X,Y,w,b):
      w_gradient = 2 * np.average(X * (predict(X,w,b) - Y))
      b_gradient = 2 * np.average(    (predict(X,w,b) - Y))
      return (w_gradient, b_gradient)
#+end_src

#+begin_src python
  def train(X,Y,iterations, lr):
      for i in range(iterations):
          print("Iteration: %4d => Loss: %.10f" %
                (i, loss(X,Y,w,b)))
          w_gradient, b_gradient = gradient(X,Y,w,b)
          w -= w_gradient * lr
          b -= b_gradient * lr
      return w, b

  w, b = train(X,Y,iterations=200,lr=0.001)
  print("|nw=%.10f, b=%.10f" % (w,b))
  print("Prediction: x=%d => y=%.2f" %
        (20, predict(20,w,b)))
#+end_src
