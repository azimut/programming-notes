* 17 | PostgreSQL Up & Running
** 5 Data types
*** Full Text Search

- a search happens between 2 vectors
  - a vectorized searched text
  - a vectorized search term

- could equate on
  * related words (ex: love, romance, infatuation, lust)
  * common stem (ex: love, loving, loved)
  * principle parts of a verb (ex: eat, eats, ate, eaten)
  * synonims

- prunning stop words

- tsvector:
  - is a type
  - is a lexeme
  - has at least one "position"

- trigger example to keep tsvector column named "fts", upto date on changes (does NOT support weighting)

  #+begin_src sql
    create trigger trig_tsv_film_iu
      before insert or update of title, desription
      on film
      for each row
        execute procedure tsvector_update_trigger(fts, 'pg_catalog.english', title, description);
  #+end_src

- ranking search results

  #+begin_src sql
    select title, left(description,50) as description,
           ts_rank(fts,ts)::numeric(10,3) as r
      from flim, to_tsquery('english', 'love & (wait | indian | mad)') as ts
     where fts @@ ts and title > ''
     order by r desc;
  #+end_src

- ranking with weights

  #+begin_src sql
    select left(title,40) as title,
           ts_rank('{0,0,0,1}'::numeric[],fts,ts)::numeric(10,3) as r,
           ts_rank_cd('{0,0,0,1}'::numeric[],fts,ts)::numeric(10,3) as rcd
      from film, to_tsquery('english', 'love & (wait | indian | mad )') as ts
     where fts @@ ts and title > ''
     order by r desc;
  #+end_src

* 19 | The Art of PostgreSQL by Dimitri Fontaine
|-----------------------+----------------------------------------------------------------------|
| Authors Blog          | https://tapoueh.org/                                                 |
| Markus Winand         | https://use-the-index-luke.com/                                      |
| Markus Winand         | https://modern-sql.com/                                              |
|-----------------------+----------------------------------------------------------------------|
| gui                   | https://www.pgadmin.org/                                             |
| gui                   | https://github.com/OmniDB/OmniDB                                     |
| dataset (about music) | https://github.com/lerocha/chinook-database                          |
| python library        | https://github.com/honza/anosql                                      |
| python library        | https://github.com/nackjicholson/aiosql                              |
| creates testing env   | http://manpages.ubuntu.com/manpages/trusty/man1/pg_virtualenv.1.html |
| sql regression tester | https://github.com/dimitri/regresql                                  |
| sql regression tester | https://pgtap.org/                                                   |
|                       | https://pgtap.org/pg_prove.html                                      |
|                       | https://pgtap.org/documentation.html#canyourelate                    |
|-----------------------+----------------------------------------------------------------------|
** About
- Doing more SQL, primarily it means writing fewer lines of code
- We should refer loc not as "lines produced" but as "lines spent" (Dijkstra)
** Part I
- PosgreSQL is an ORDBMS (Object-Oriented Relation Database Manager)
** Part II   Introduction
*** 1 SQL Structured Query Language
#+begin_src
 Bad programmers worry about the code.
 Good programmers worry about data structures and their relationships

   Linus Torvalds
#+end_src
- We declare the result in terms of a ~data processing pipeline~
  that is executed against a known ~database model~ and dataset
- The database model is statically typed
- RDBMS and SQL are forcing developers to think in terms of a ~data structure~
- SQLite project is the "most widely deployed software module of any type" (along with zlib, libpng, libjpeg)
- The current SQL standard is SQL'2016
**** A first use case - loading a .csv
1) CSV
  #+begin_src
 2010 1/4/2010 1,425,504,460 4,628,115 $38,495,460,645
 2010 1/5/2010 1,754,011,750 5,394,016 $43,932,043,406
 2010 1/6/2010 1,655,507,953 5,494,460 $43,816,749,660
 2010 1/7/2010 1,797,810,789 5,674,297 $44,104,237,184
  #+end_src
2) Load the CSV, into an ad-hoc table and then tranform it into proper sql data types, thanks to the alter table commands
  #+begin_src sql
    begin;
    create table factbook (
      year    int,
      date    date,
      shares  text,
      trades  text,
      dollars text
    );
    \copy factbook from 'factbook.csv' with delimiter E'\t' null '';
    alter table factbook
      alter shares  type bigint using replace(shares, ',', '')::bigint,
      alter trades  type bigint using replace(trades, ',', '')::bigint,
      alter dollars type bigint using substring(replace(dollars, ',', '') from 2)::numeric;
    commit;
  #+end_src
3) Query - \set start '2017-02-01'
  #+begin_src sql
    select date,
           to_char(shares, '99G999G999G999') as shares,
           to_char(trades, '99G999G999') as trades,
           to_char(dollars, 'L99G999G999G999') as dollars
      from factbook
     where date >= date :'start'
       and date  < date :'start' + interval '1 month'
     order by date;
  #+end_src
**** A word about SQL injections
- PSQL implements a *protocol* level facility to send the static SQL query text,
  separetely from its dynamic arguments.
  https://www.postgresql.org/docs/current/protocol-flow.html#PROTOCOL-FLOW-EXT-QUERY
  https://www.postgresql.org/docs/current/libpq-exec.html#LIBPQ-PQEXECPARAMS (part of the libpq C Driver)
- Never build a query string by concatenating your query arguments directly into your query strings.
- ~psycopg~, linked to libpq, the sql query parameters are interpolated in the SQL query (client). We trust psycopg to protect us.
- ~asyncpg~, no linked to libpq, uses server-side prepared statements https://github.com/MagicStack/asyncpg
- JDBC, Go's pq packages do NOT link to libpq
- In the background, a *server side prepared statement* it does (PREPARE, EXECUTE)
  #+begin_src sql
    prepare foo as
      select date, shares, trades, dollars
        from factbook
       where date >= $1::date
         and date  < $1::date + interval '1 month'
       order by date;
    execute foo('2010-02-01');
  #+end_src
**** Back to Discovering SQL (our use case)
1) Code to fill for the days with missing data.
   #+begin_src sql
     select cast(calendar.entry as date) as date,
            coalesce(shares, 0) as shares,
            coalesce(trades, 0) as trades,
            to_char(coalesce(dollars, 0), 'L99G999G999G999') as dollars
       from generate_series(date :'start',
                            date :'start' + interval '1 month' - interval '1 day',
                            interval '1 day') AS calendar(entry)
            left join factbook
                on factbook.date = calendar.entry
      order by date;
  #+end_src
2) Down the road, that's less code to maintain and more efficient implementation too.
3) Create a wekk over week percentage difference
   WITH: a *common table expression*
   LAG: Provides access to rows before
     OVER
     PARTITION BY
   EXTRACT: extracts subfields from a timestamp
   CASE/WHEN/THEN
  #+begin_src sql
    WITH computed_data AS
    (
      SELECT CAST(date as date) as date,
      TO_CHAR(date, 'Dy')       as day,
      COALESCE(dollars, 0)      as dollars,
      LAG(dollars, 1)
        OVER(
          PARTITION BY EXTRACT('isodow' from date)
              ORDER BY date
         )
       AS last_week_dollars
     FROM
       GENERATE_SERIES(date :'start' - interval '1 week',
                       date :'start' + interval '1 month'
                                     - interval '1 day',
                       interval '1 day'
       )
       AS calendar(date)
       LEFT JOIN factbook USING(date)
    )
    SELECT date, day,
           TO_CHAR(
             COALESCE(dollars, 0),
             'L99G999G999G999'
           ) AS dollars,
           CASE WHEN dollars IS NOT NULL
                 AND dollars <> 0
                THEN ROUND(   100.0
                           ,* (dollars - last_week_dollars)
                           / dollars
                         , 2)
           END
           AS "WoW %"
        FROM computed_data
       WHERE date >= date :'start'
     ORDER BY DATE;
  #+end_src
*** 2 Software Architecture
- Think it not as a "storage layer" bur rather as "concurrent data access service"
- Book focused on
  - SQL idioms
  - Database Modeling
  - Normalization
  - Denormalization
*** 3 Getting Ready to read this book
** Part III  Writing SQL Queries
- How to write queries, as part fo your application code
- Is SQL a good place to implement business logic?
*** 4 Business Logic
- SQL How much on the database?
  #+begin_src sql
    select name
      from track
     where albumid = 193
  order by trackid;
  #+end_src
- SQL Adding genre table
  #+begin_src sql
    select track.name as track, genre.name as genre
      from track
      join genre using (genreid)
     where albumid = 193
  order by trackid;
  #+end_src
- SQL Adding some computation to the values returned
  #+begin_src sql
    select name,
           milliseconds * interval '1 ms' as duration
      from track
     where albumid = 193
  order by trackid;
  #+end_src
- SQL Calculating the album length per artist given
  #+begin_src sql
     select album.title as album,
            sum(milliseconds) * interval '1 ms' as duration
       from album
            join artist using(artistid)
            left join track using(albumid)
      where artist.name = 'Red Hot Chili Peppers'
   group by album
   order by album;
  #+end_src
- *application_name* put into the connection string
   Sets the application name to be reported in statistics and logs.
- Correctness
  Transaction Isolation https://www.postgresql.org/docs/current/transaction-iso.html
  1) Read uncommited ?
  2) Read committed: default, you will see changes as soon as they happen
  3) Repeatable read: keeps a snapshot for each transaction (between a BEGIN and COMMIT) useful for online backups
  4) Serializable: ? eg: in stock managment facilities
- Efficiency:
  * Static: development time, maintenence burden, how easy is to review the code.
  * Dynamic: resources, processor, memory, network, disk
- ~When doing very simple queries against *primary key* column, it's quite common to see 0.1ms execution time~
- ~Write stored procedures in *SQL*, only switch to *PLpgSQL* when necessary~
**** =Stored Procedures=: allows us to build a data access API
  * Naive:
    #+begin_src sql
    create or replace function get_all_albums
    (
      in  name     text,
      out album    text,
      out duration interval
    )
    return setof record
    language sql
    as $$
      select album.title as album,
             sum(milliseconds) * interval '1 ms' as duration
        from album
             join artist using(artistid)
             left join track using(albumid)
        where artist.name = get_all_albums.name
    group by album
    order by album;
    $$;
    #+end_src
  * More efficient version that uses album_id (see above about primary key lookups)
      #+begin_src sql
      create or replace function get_all_albums
      (
        in  artistid bigint,
        out album    text,
        out duration interval
      )
      returns setof record
      language sql
      as $$
        select album.title as album,
          from album
               join artist using(artistid)
               left join track using(albumid)
          where artist.artistid = get_all_albums.artistid
      group by album
      order by album;
      $$;
      #+end_src
  * Calling it
   #+begin_src sql
   select * from get_all_albums(127);
   -- OR by name using a subquery
   select *
     from get_all_albums(
       (select artistid
          from artist
         where name = 'Red Hot Chili Peppers')
     );
   -- OR using lateral join
   select album, duration
     from artist,
          lateral get_all_albums(artistid)
    where artist.name = 'Red Hot Chili Peppers';
   #+end_src
  * Only of artists with 4 albums
      #+begin_src sql
    with four_albums as
    (
       select artistid
         from album
     group by artistid
       having count(*) = 4
    )
       select artist.name, album, duration
         from four_albums
              join artist using(artistid),
              lateral get_all_albums(artistid)
     order by artistid, duration desc;
    #+end_src
  * Procedural Code vs Stored Procedures
    We can rewrite the previous as a stored procedure, but it will be ugly.
*** 5 A Small application

- Load the chinook dataset with pgloader, we get a summaryof rows added per table
  #+begin_src
  $ createdb chinook
  $ pgloader https://github.com/.../Chinook_Sqlite_AutoIncrementPKs.sqlite
  psql > ALTER TABLE track
         ADD PRIMARY KEY USING INDEX idx_51519_ipk_track;
  #+end_src

- Counting the number of tracks per genre
  #+begin_src sql
    select genre.name,
           count(*) as count
      from genre
           left join track using(genreid)
     group by genre.name
     order by count desc;
  #+end_src

- using python libraries anosql, aisql libraries, keep SQL files separate
  #+NAME: artist.sql
  #+begin_src sql
    -- name: top-artists-by-album
    -- Get the list of the N artist with the most albums
    select artist.name, count(*) as albums
      from artist
           left join album using(artistid)
     group by artist.name
     order by albums desc
     limit :n;
  #+end_src

- This file format, is also readable by pgsql shell
  \set n 1
  \i artist.sql
  \set n 3
  \i artist.sql

- Or from shell
  $ psql --variable "n=10" -f artist.sql chinook

- LEFT JOIN LATERAL:
    We use lateral join again, to get some kind of "nested loops".
    We use genre from outside the subquery.
    We correlate genre between the outer loop and inner loop.
    - https://stackoverflow.com/questions/28550679/what-is-the-difference-between-lateral-join-and-a-subquery-in-postgresql
          "For returning more than one column, a LATERAL join is typically simpler, cleaner and faster."
          "A correlated subquery can only return a single value, not multiple columns and not multiple rows"
  ON TRUE: "the joins happens on the sub-query WHERE clause, we don't need another OUTER JOIN"
  #+begin_src sql
    select genre.name as genre,
           case when length(ss.name) > 15
             then substring(ss.name from 1 for 15) || '...'
           else ss.name
           end as track,
           artist.name as artist
      from genre
           left join lateral
           (
             select track.name, track.albumid, count(playlistid)
               from track
                    left join playlisttrack using (trackid)
              where track.genreid = genre.genreid
              group by track.trackid
              order by count desc
              limit :n
           ) ss(name, albumid, count) on true
           join album using(albumid)
           join artist using(artistid)
     order by genre.name, ss.count desc;
  #+end_src
*** 6 The SQL REPL - An Interactive Setup
    - \set ON_ERROR_ROLLBACK on/off/interactive
      Useful when working with BEGIN transaction on *psql*
      *interactive* Allows us to COMMIT on some error
    - We can run a query and return the results on:
      \pset format
      - asciidoc
      - HTML
      #+begin_src
      psql --tuples-only    \
           --set n=1        \
           --set name=Alesi \
           --no-psqlrc      \
           -P format=html   \
           -d f1db          \
           -f report.sql
      #+end_src
    - Use the connection string directly
      #+begin_src
      psql -d postgresql://dim@localhost:5432/f1db
      psql -d "user=dim host=localhost port=5432 dbname=f1db"
      #+end_src
    - psql schema commands, do queries to *catalog* in the background
      ~\set ECHO_HIDDEN true~ to show scheme queries done by psql for you
*** 7 SQL is Code
**** Style Guidelines
- Follow the ~Principle Of Least Astonishment~ rule
  a.k.a. POLA
  https://en.wikipedia.org/wiki/Principle_of_least_astonishment
- "old habit of all-caps keywords", not needed with syntax highlighting
- Right aligned
- JOIN - ON vs USING
- Old habit of
  "writing the join conditions of INNER JOIN in the WHERE clause"
  "confusing and frowned upon"
  #+begin_src sql
  SELECT name, title
    FROM artist, album
   WHERE artist.artistid = album.artistid
     AND artist.artistid = 1;
  #+end_src
- Modern spelling, expanded the INNER JOIN to his full notation
  #+begin_src sql
    select name, title
      from artist
           inner join album using(artistid)
     where artist.artistid = 1;
  #+end_src
- https://stackoverflow.com/questions/17759687/cross-join-vs-inner-join-in-sql
- ~noise words~, INNER and OUTER
  - OUTER: left join, right join, full join
  - INNER: join
- NATURAL JOIN: "automatically expand a join condition over columns having the same name"
  should be avoided
- We can use the same table twice.
**** Unit testing

- tools: pg_virtualenv, pgtap, regresql

- https://julien.danjou.info/db-integration-testing-strategies-python/
  Python

- The approach one used by postgresql
  https://github.com/postgres/postgres/blob/master/src/test/regress/sql/aggregates.sql
  https://github.com/postgres/postgres/blob/master/src/test/regress/expected/aggregates.out
  1) psql: Run a SQL file containing your tests
  2) Capture its output to a text file that includes the queries and their results
  3) diff: Compare the output with the expected one that is maintained in the repository
  4) Report any difference as failure

*** 8 Indexing Strategy
- "Indexing strategy" for ~Speed~
  - In the absence of an index, the only option available
    to your database is *sequential scan* of your tables.
- "Indexing strategy" for ~Consistency~
  if used to ensure data consistency, is a data modeling activity
  - Constrains
    - UNIQUE
    - PRIMARY KEY
    - EXCLUDE USING
- M.V.C.C. Multi-version Concurrency Control
- An index duplicates data in a specialized format made to optimize certain searches
  - Adds *write cost* to your DML insert/update/delete, as it needs to maintain the index up to date
- Index types:
  1) B-Tree (default), handle concurrent read and write
     https://github.com/postgres/postgres/tree/master/src/backend/access/nbtree
  2) Hash
     - Simple equality comparisons
     - >10 are crash safe
  3) GiST (generalized search tree), content-based indexing for massive amounts of complex content
     - Support for 2D data types (geometry point or ranges)
  4) SPGiST (spaced partitioned gist)
     - support non-balanced disk-based data structures (index 2D data with different densities)
       - quadtrees
       - k-d trees
       - radix trees (tries)
  5) GIN (generalized inverted index)
     - Foundation for PSQL *full text search* support
       https://www.postgresql.org/docs/current/textsearch-intro.html
     - when items to be indexed are *composite values*, and the queries search for elements that appear within the composite item
  6) BRIN (block range indexes)
     - store summaries about the values stored in consecutive physical block ranges
     - For data types that have order, it can index the minimum and maximum values in the colum for each block range
  7) Bloom Filters
     - *create extension bloom*
     - Space efficient
     - Test if an elements belongs into a set
     - B-trees are faster
     - Only supports *equality*
     - Used when too many B-Tree would be needed otherwise
- Advanced
  https://www.postgresql.org/docs/10/indexes.html
- pg_stat_statements: check >10ms
  https://www.postgresql.org/docs/current/pgstatstatements.html
  - List the most common queries in
    - number times
    - cumulative time it took to execute
- EXPLAIN usage
  - explain (analyze, verbose, buffers)
  - Visualizers
    - https://explain.depesz.com/
    - http://tatiyants.com/pev/#/about
    - pgAdmin comes with a visualizer
  - Check row count difference between
    - estimated
      effective
    - If not, might need to check the interval of the autovacuum
      https://www.postgresql.org/docs/current/routine-vacuuming.html#AUTOVACUUM
  - Check time spent doing *sequential scans*, with a filter step
  - https://en.wikipedia.org/wiki/Amdahl%27s_law
** Part IV   SQL Toolbox
- IN SQL you need to explain your problem,
  unlike in most programming languages where
  you need to focus on a solution.
- Try write down a single sentence that perfectly describes
  what you're trying to achieve. Talking out loud.
*** 10 Get some Data

- http://ergast.com/mrd/db/
  $ createdb f1db
  $ pgloader mysql://root@localhost/f1db pgsql:///f1db

- Tweak PSQL search_path to include f1db *schema* in the f1db *database*
  #+begin_src sql
  ALTER DATABASE f1db SET search_path TO f1db, public;
  #+end_src

*** 11 Structured Query Language

- For some developers, not being in charge of every detail
  of the query plan is a source of *frustation*, and they
  prefer hiding SQL under another layer of technology that
  makes them feel like they are still in control.

*** 12 Queries, DML, DDL, TCL, DCL

- SQL is composed of several areas
  1) DML Data Manipulation Language
     - insert
     - update
     - delete
  2) DDL Data Definition Language (data-structures)
     - create
     - alter
     - drop
  3) TCL Transaction Control Language
     - begin
     - commit
     - rollback
     - start transaction
     - savepoint, release savepoint, rollback to savepoint
     - commit, prepare commit, commit prepared, rollback commit
  4) DCL Data Control Language
     - grant
     - revoke
  5) Other:
     - vacuum, analyze, cluster
     - prepare, execute, explain, listen, notify, lock, set

*** 13 Select, From, Where
**** Projection (output) aka SELECT

- Is usually frowned upon to use "SELECT *" aka the classic /I don't know what I'm doing/
- It can give worst/unclear error messages in case of schema changes
- Behavior of some ORMs (Object Relational Mappers) when they
  insist of fully /hydrating/ the application objects, just in case
  (?)
  #+begin_src sql
    select * from races limit 1;
    select * from races fetch first 1 rows only; -- part of the "sql standard"
    table rages limit 1; -- part of the "sql standard"
  #+end_src

- Using postgres with Java
  $ javac Select.java
  $ java -cp .:path/to/postgresql-42.1.1.jar Select

- Using ~select star~ also brings problems with application code
  - Hides the intention
  - Is not efficient to retrieve fields you don't need
    - postgresql can't return rows longer than 8k without doing some external table pointers
      - TOAST mechanism, makes some bytes expensive to retrieve
        (T)he
        (O)versized
        (A)ttribute
        (S)torage
        (T)emplate
        https://www.youtube.com/watch?v=_UUFMAZswhU

**** Data Sources: FROM

- FROM introduces data sources, and how they relate to each other

- Example: top 3 drivers
  #+begin_src sql
      select code, forename, surname,
             count(*) as wins
        from      drivers
             join results using(driverid) -- inner join
       where pos = 1
    group by driverid
    order by wins desc
       limit 3;
  #+end_src


- FROM, types of JOIN https://www.postgresql.org/docs/current/queries-table-expressions.html#QUERIES-FROM
  |--------------------+----------------------------------------------------------|
  | CROSS JOIN         | N*M for every possible combination of rows               |
  | FROM t1,t2         | "                                                        |
  | (INNER) JOIN t2 ON | N*M, that satisfies ON condition                         |
  | (OUTER) LEFT  JOIN | inner join + t1 rows that didn't satisfy ON condition    |
  | (OUTER) RIGHT JOIN | inner join + t2 rows that didn't satisfy ON condition    |
  | (OUTER) FULL  JOIN | inner join + t1,t2 rows that didn't satisfy ON condition |
  |--------------------+----------------------------------------------------------|

**** Restrictions: WHERE

- WE usually try to keep the *WHERE* clauses as simple,
  in order to be able to use our ~indexes~.

- *OR* operator is more complex to optimize, in respect to ~indexes~

- Careful with *NOT IN* and NULL
  Example: this returns no rows
  #+begin_src sql
    select x
      from generate_series(1,100) as t(x)
     where x not in (1,2,3,null);
  #+end_src

- We can use sub-queries on WHERE, to implement the
  "anti-join" pattern using the NOT EXISTS and SELECT 1
  #+begin_src sql
  select forename,
         surename,
         constructs.name        as constructor,
         count(*)               as races,
         count(distinct status) as reasons
    from drivers
   where date >=
  #+end_src

** Part V    Data Types
** Part VI   Data Modeling
** Part VII  Data Manipulation and Concurrency Control
** Part VIII PostgreSQL Extensions
** Part IX   Closing Thoughts
