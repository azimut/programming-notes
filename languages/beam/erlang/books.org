- https://github.com/ericdouglas/distributed-systems-journey
- https://github.com/zkessin/testing-erlang-book
- https://github.com/inaka/erlang_guidelines
- https://adoptingerlang.org/
- https://github.com/oreillymedia/etudes-for-erlang
- https://github.com/heroku/erlang-in-anger
- beam https://github.com/happi/theBeamBook
- beam http://beam-wisdoms.clau.se/en/latest/
* Cowboy User Guide (2.6)
https://ninenines.eu/docs/en/cowboy/2.6/guide/
** Rationale
*** The modern Web
+ HTTP/2:
  - RFC 7540
  - RFC 7541
  - binary protocol
  - enables clients to keep a connection open for long periods of time
  - to send requests concurrently
  - to reduce the size of requests throuhg *http headers compression*
  - enables server to client message push
+ HTTP/1.1
  - text based protocol
  - difficult to detect if a client disconnected
+ Websocket
  - RFC 6455
  - binary based protocol
  - build on top of HTTP/1.1
  - closer to TCP, it requires you to design and implement your own protocol on top of it
  - provides a two-ways communication channel
  - asynchronous and concurrent
  - support for small and big data transfers
+ Long-polling
  - For requests that may not be immediately anaswered by the server.
  - A hack to server older client and servers
  - media type: "text/event-stream"
    http header: "Last-Event-ID"
*** Erlang and the Web
- Each time you connect to a website you open a couple of connections,
  multiply that by each person that connects to the site.
- "C10K problem" aka ten thousand concurrent connections problem
- soft real time (the Web is)
  operations need to be done as quickly as possible
- hard real time:
  operations need to be done in under N milliseconds otherwse the sytem fails entirely
- asynchronous (the Web now is)
  - that is since the additions of websockets and http2
- "Erlang developers...can focus on handling only the errors that should give
  some feedback to the user and let the system take care of the rest."
** Introduction
*** Introduction
- header names
  - by spec, are case insensitive
  - cowboy http/1.1, converts all the requests to lowercase
  - cowbow http/2.0, requires the clients to send them as lowercase
*** Getting started
- Tutorial uses *Erlang.mk* build system
  https://github.com/ninenines/erlang.mk
- $ mkdir hello_erlang
  $ cd hello_erlang
  $ wget https://erlang.mk/erlang.mk
- Creates a makefile, a base application and a release files
  $ make -f erlang.mk bootstrap bootstrap-rel
  $ make run
  > q().
**** Makefile
#+begin_src makefile
PROJECT = hello_erlang
DEPS = cowboy
dep_cowboy_commit = 2.6.3
DEP_PLUGINS = cowboy # plugin includes templates
include erlang.mk
#+end_src
**** src/hello_erlang_app.erl
- ake done at application startup
- "/" goes to "hello_handler" module
#+begin_src erlang
start(_Type, _Args) ->
    Dispatch = cowboy_router:compile([{'_', [{"/", hello_handler, []}]}]),
    {ok, _} = cowbow:start_clear(my_http_listener,
                                [{port, 8080}],
                                #{env => #{dispatch => Dispatch}}),
    hello_erlang_sup:start_link().
#+end_src
**** src/hello_handler.erl
- generated by doing
  $ make new t=cowboy.http n=hello_handler
#+begin_src erlang
init(Req0, State) ->
    Req = cowbow_req:reply(200,
                           #{<<"content-type">> => <<"text/plain">>},
                           <<"Hello Erlang!">>,
                           Req0),
    {ok, Req, State}.
#+end_src
*** Flow diagram
- Cowboy is buld on top of Ranch
- Ranch handles the network connections
- client
  -> acceptor (handled by Ranch)
  -> protocol
  <-> stream (created by Cowboy)
  <-> middlewares (router -> handler)
- by default Cowboy comes configured with a
  =stream= handler caled *cowboy_stream_h*.
  - will create a new process for every request coming in
    and communicate with the process to read the body or send a response back
- A response may be sent at almost any point in the diagram.
- Cowboy generates one *Date* header value every second,
  shared to all other processes.
** Configuration
*** Listeners
Cowboy provides 2(two) types of listeners.
**** 1) Clear TCP Listener
- See the example used above on "Getting started"
**** 2) Secure TLS listener
#+begin_src erlang
  start(_Type, _Args) ->
      Dispatch = cowboy_router:compile([{'_', [{"/", hello_handler, []}]}]),
      {ok, _} = cowboy:start_tls(my_http_listener,
                                 [{port, 8443},
                                  {certfile, "/path/to/certificate"},
                                  {keyfile, "/path/to/keyfile"}],
                                #{env => #{dispatch => Dispatch}}),
      hello_erlang_sup:start_link().
#+end_src
*** TODO Routing
- Cowboy does nothing by default
- You need to map URIs to Erlang Modules that will handle the requests.
- *routes* need to be compiled before they can  be used.
  The result of the compilation is the *dispatch* rules.
#+begin_src c
  Routes = [Host1, Host2, ...HostN]
  Host1 = {HostMatch,              PathsLists}
  Host2 = {Hostmatch, Constraints, PathsLists}

  PathsList = [Path1, Path2, ...PathN]
  Path1 = {PathMatch,              Handler, InitialState}
  Path2 = {PathMatch, Constraints, Handler, InitialState}
  #+end_src
**** Match Syntax
- Can be given as either a string() or binary()
- ":" to =bind= segments,
  - Later retrieved with *cowboy_req:binding/{2,3}*
  - If used the same twice, will match if equal.
    Even if used once in Host and once in Path
- :_" as a bind atom, data is discarded
- [] for =optional= segments
  - can be nested
  - undefined behaviour if multiple
- [...], match anything before(hosts) or anything after(paths)
  - *cowboy_req:host_info/1*
- "_" matches any
- Paths
  - should start with "/"
  - "/" at the end is the same as not putting it
- Hosts
  - "." at the beggining is the same as not putting it
*** Constraints
- Are validation and conversion functions applied to user input
- Should be written to NOT emit any exceptions
- Provided as a list of *fields*
- A field can take the form
  - atom
  - {atom, Constraints} (only valid one with router)
  - {atom, Constraints, Default}
- Constraints are provided as an ordered list of (or a single atom if 1(one))
  - atoms (the build-int ones, eg: int, nonempty)
  - or fun/s to apply
    #+begin_src erlang
      PositiveFun = fun
                        (_Op, Val) when Val > 0 -> {ok, Val};
                        (_Op, _) -> {error, not_positive}
                    end,
      {my_value, [int, PositiveFun]}
    #+end_src
- Operations
  - forward: operations used for validating and converting user input
  - reverse: it takes a converted value and changes it back to what the user input would have been
  - format_error: takes an error returned by other operation and returns a formatted human-readable message
** Handlers
*** Handlers
*** Loop handlers
*** Static files
** Request and response
*** Request details
*** Reading the request body
*** Sending a response
*** Using cookies
*** Multipart
** REST
*** REST principles
*** Handling REST requests
*** REST flowcharts
*** Designing a resource handler

** Websocket

*** The Websocket protocol
*** Websocket handlers

** Advanced

*** Streams
*** Middlewares

* Adopting Erlang
https://adoptingerlang.org/
** OTP Applications
- Every component to be shipped in an Erlang/OTP release
  needs to be an OTP application.
- $ rebar3 new help lib
  $ rebar3 new lib mylib desc="Checking out OTP libs"
  $ rebar3 compile
*** rebar3 templates
- app
  - can be used as a dependency
  - can be turned into a full release
  - stateful process
- lib
  - can be used as a dependency
  - a collection of modules
  - no process
- cmake
- escript
  - requires erlang to be installed by the user
- release
- umbrella
- plugin
  - rebar3 plugin structure
*** _build/
- everything under it is split by profile
  https://www.rebar3.org/docs/profiles
  https://rebar3.org/docs/configuration/profiles/
- _build/default/lib/
  | dep1/                 | a directory for each dependency or                             |
  | mylib/                | a directory for each app                                       |
  | mylib/ebin/mylib.app  | final .app created from the .src                               |
  | mylib/ebin/mylib.beam | compiled version of the .erl files                             |
  | mylib/include/        | symlink to parent dir                                          |
  | mylib/priv/           | symlink to parent dir                                          |
  | mylib/src/            | symlink to parent dir                                          |
*** .src
- {module,[]} is auto filled by rebar3
- {applications,[]}
  - indicates the ORDER in which applications must be started to work
  - helps build a dependency graph
- {mod {<appname>_app, []}}
  - for applications, not libraries
  - [] are the arguments
  - when called is supposed to return the PID of the supervision tree
** Supervision Trees
- In OO languages
  - We would tend to pick where side effects live
    https://en.wikipedia.org/wiki/Domain-driven%5Fdesign
  - While trying to respect
    - Persistence Ignorance https://deviq.com/principles/persistence-ignorance
    - hexagonal Architecture https://fideloper.com/hexagonal-architecture
      "Allow an application to equally be driven by users, programs,
      automated test or batch scripts, and to be developed and tested
      in isolation from its eventual run-time devices and databases."
- On OTP, the richness of failure and fault handling,
  will be explicitly encoded within a *supervision structure*.
  - Stateful parts of the system are encoded using processes
  - processes are started depth-first, from left to right
  - processes are stopped depth-first, from right to left
  - https://adoptingerlang.org/img/suptree.png
- It's about the guarantees
  1) There isn't a backoff or cooldown period before a *supervisor* restarts a crashed child.
     - If you setup a connection to an external service that might be down on initialization.
       The *application* will fail to boot after many restarts.
  2) Restarting a *process* is about bringing it back to a stable, known state.
     From there thing can be retried.
  3) An initialized *process* should be stable not matter what happens.
  4) Supervised *processes* provide guarantees in their initialization phase, not best effort.
     - If you are writting a client for a DB or Service, you shouldn't need a connection to be established
       as part of the initialization phase unless you're ready to say:
       "It will always be available no matter what happens"
     - It's acceptable if the DB it runs on the same host. And should boot before the *application*
     - If it is on a remote host, you should expect the connection to fail.
       You can guarantee being able to communicate to the client, but not the database.
       IE: return {error, not_connected}
     - Reconnection can be done using cooldown/backoff strategies.
       Should be able to reconnect if anything happens later.
  5) Supervisors are about restarts, but they should be about restarts to a stable known state.
- Growing Trees
  - Everything you feel is *fragile* and should be allowed to fail has to move deeper into the hierarchy.
  - What is *stable* and *critical* needs to be reliable is higher up.
  - If all the childs and their supervisor are marked as =permanent=,
    then there is a possibility that frequent crashes will take down the whole node.
  - _The manager worker pattern_
    1) mark the workers as =permanent= or =transient=, so they get restarted
    2) mark the workers direct *supervisor* as =temporary=, so if it fails it gives up
    3) add a new *supervisor* above it, make it a *one_for_one*
    4) add a new *process* under the NEW *supervisor*, link the process to the OLD *supervisor*
       - this will be the "manager"
       - it can apply any policy it wants when the linked supervisor dies
         * could wait until conditions for restart are met
         * and then ask the parent supervisor to restart the temporary supervisor
  - We want to keep *supervisors* as simple and as predictable as they can be
- Questions to ask yourself about your application supervisor tree
  1) It is okay if this dies?
  2) Should other processes be taken down with this one?
  3) Does this process depend on anything else that will be weird once it restarts?
  4) How many crashes are too many for this supervisor?
  5) if this part of the system is flat out broken, should the rest of it keep working or should we give up?
*** What's in a supervisor
- single init/1 callback, to define their children
- ChildSpect arguments
  | id       |                  | mandatory                                                                          |
  | start    |                  | mandatory, the {M,F,A}                                                             |
  | restart  | permanent        | *permanent* or *transient* or *temporary*                                          |
  | shutdown | 5000 or infinity | brutal_kill or infinity or number of milliseconds to wait for a proper termination |
  | type     | worker           | only used when doing live upgrades, worker or supervisor                           |
  | modules  | M from {M,F,A}   | only used when doing live upgrades                                                 |
- 3 Children Policies
  1) supervisor type: about error propagation, from child to sup
   | one_for_one        |                                    |
   | simple_one_for_one | children of the same type 1        |
   | rest_for_one       | those started after get restarted  |
   | one_for_all        | if any dies, all must be restarted |
  2) restart policy of a child
    |           | normal exit | abnormal exit |
    |-----------+-------------+---------------|
    | permanent | restart     | restart       |
    | transient | -           | restart       |
    | temporary | -           | -             |
  3) frequency of failures to be accepted
    | intensity | how many crashes have been spotted |
    | period    | during how many seconds            |
- Bohrbug: a bug solid, observable, and easily repeatable.
  Heisenbug: unreliable behaviour, manifests under certain conditions
- Given a system with 100,000 request a second
  - once in a B bug, once every 3 hours
  - once in a M bug, once every 10 seconds
  - but their occurrence wouls still be rare in tests
** Dependencies
*** Programming practices
  1) BEFORE:
     focus on writing reusable and extendable components within each project
  2) CURRENT:
     make small and isolated projects that each individually ends up being
     easy to throw away and replace.
*** Rebar3
  - is a declarative build tool.
  - can run custom scripts and extend the build https://www.rebar3.org/docs/configuration/plugins/
    - plugins are NOT version locked automatically
  - Dynamic configuration http://rebar3.org/docs/configuration/config_script/
    - with rebar.config ad *.app.src
      based on file:script/2
  - Commands
    - tree: ascii tree of dependencies
    - upgrade: updates the rebar.lock
    - get-deps > compile > release > tar
  - all *build-time* dependencies (and deps of those deps) are fetched to the project's _build/default/lib/ directory
  - *run-time* dependencies go into "application" in the .app.src file
    - erlang.mk and Mix, copy the build-time dependencies for you into the final "applications" for you
      though, they have other mechanism for more fine grained control of dependencies
  - for dependencies declared multiple times, it ends up using the closes to the projects root
  - warnings are emitted during the first build
  - default profiles
    - default
    - test
    - docs
    - prod
  - It does NOT support relative paths to declare libraries
*** Dependencies
  #+begin_src erlang
    {deps, [
            {AppName, {git, "http...", {tag "1.2.0"}}},
            {AppName, {git, "http...", {branch "master"}}},
            {AppName, {git, "http...", {ref "aead1231...."}}},
            AppName, % latest known version
            {AppName, "1.2.0"},
            {AppName, "~> 1.2.0"}, % 1.2.0 or above, but below 1.3.0
            {ApName, "1.2.0", {pkg, PkgName}}, % when AppName is published with package name PkgName
  #+end_src
*** Dependencies Lifecycles
  - Remove an unused dependency from the lockfile, after removed from rebar.config
    $ rebar3 unlock <appname>
  - Diregard the version on the lockfile, and rebuild the dependency tree
    $ rebar3 upgrade <appname>
  - Updates known Hex versions
    $ rebar3 update
  - List dependencies, and tells which can be updated.
    $ rebar3 deps
*** _checkout/ Dependencies
  - For use when you are developing locally and don't want to "modify the dependency, publish it, build and pull the dependency"
  - Every time the main app is build, it will add  an ebin/ directory to the dependency directory in _checkouts/
  - How?
    - add a _checkouts/ directory to the main project
    - copy or symlink the dependency directory in the _checkouts/ directory
    - once done and dependency is published, delete _checkouts/ and run "rebar3 upgrade"
*** Using Elixir (Hex) Dependencies
1) Add the rebar_mix plugin to your rebar.config
   #+begin_src erlang
     {plugins, [rebar_mix]}.
     {provider_hooks, [{post, [{compile, {mix, consolidate_protocols}}]}]}.
     {relx, [
             % ...
             {overlay, [{copy,
                         "{{base_dir}}/consolidated",
                         "releases/{{release_version}}/consolidated"}]}
             ]}.
   #+end_src
2) Add this to your *vm.args.src* file
   #+begin_src sh
     -pa releases/${REL_VSN}/consolidated
   #+end_src
*** Corporate Environments
1) HTTP_PROXY and HTTPS_PROXY env variable support
2) You can setup your own private Hex mirror http://blog.plataformatec.com.br/2019/07/announcing-minirepo-a-minimal-hex-server/
3) A rebar plugin to specify path dependencies.  https://github.com/benoitc/rebar3_path_deps
** Multi-App Projects
- aka umbrella projects
- used because they are easier to maintain multiple OTP applications within a single repository
- requires an /apps directory (can also be called /libs)
- use some sort of "namespacing" while naming since the vm doesn't support that
  - service_discovery/apps/service_discovery
    service_discovery/apps/service_discovery_grpc
    service_discovery/apps/service_discovery_http
    service_discovery/apps/service_discovery_postgres
    service_discovery/apps/service_discovery_storage
  - they could have different names
    - vendoring some library, we would have: authlib and authlib_http
  - /src, /priv, /proto, /test
  - root level hooks will run first than an application level hook
- Caveats
  1) For libraries to be used across multiple repositories, multi-app won't work
  2) Some commands take more time to run
- Templates https://rebar3.org/docs/tutorials/templates/
** TODO Hard Things to Get Right
*** Handling Unicode
*** Handling Time
- Use cases for time measurements
  1) knowing the duration between 2(two) given events: microseconds, hours, years
     =erlang:monotonic_time/0-1=
     - returns an ever increasing value, in *microseconds*
       usually a negative number
       can take: millisecond/second(s) as an argument for the unit to measure
  2) placing an event in a timeline: usually a datetime
     =erlang:system_time/0-1=
     - in POSIX time
- UTC, hadles leap seconds, unix timestamps do not https://en.wikipedia.org/wiki/Leap%5Fsecond
  Using them interchangeable could introduce subtle bugs
- Due clock drifting https://en.wikipedia.org/wiki/Clock%5Fdrift
  we use NTP
  as such the clock might do jumps in time
** Releases
- Starting an erlang release is similar to an OS boot sequence
- =boot file= an Erlang node starts by running instructions found in it
  http://erlang.org/doc/man/script.html
- .app (Application resource file)
- .rel (Release resource file)
  - modules to load and the dependencies of each
- .rel + .app(s)  of each app
  -> systools               -> .script
  -> systools:script2boot/1 -> .boot
- History of release tools:
  1) systools
  2) reltool (bundled with OTP)
  3) relx (bundled with rebar3)
- ~sasl~ application is required by ~rebar3 release~
  ONLY IF you need release upgrades or downgrades functionality
  IF working with container images, it's not needed to be included
- _build/default/rel/service_discovery/bin/service_discovery console
- rebar3 release
  rebar3 as prod tar
  rebar3 as prod release
*** relx
- {dev_mode,true}
  causes to use symlinks for apps directories for faster development loops when starting/stopping the release
- relx section on rebar3.config
  - {release,{NAME,VERSION},APPLICATIONS}
    - VERSION can be {git,long}
    - APPLICATIONS order is important
*** Building a development release
*** Building a production release
- {profiles, [{prod, [{relx, []}]}]}
  - override:
    | dev_mode       | false       | to copy NOT symlink _build/             |
    | include_erts   | true        | to include erlang runtime               |
    | include_src    | false       | save space                              |
    | debug_info     | strip       | save space, used by debugger,xref,cover |
    | sys_config_src | path string |                                         |
    | vm_args_src    | path string |                                         |
*** Runtime configuration
*** Coming up
** Docker
- Layers
  * shared between images
  * created by each command on a Dockerfile
  * images contain many, merged at runtime
- Advantages of containers, operations we avoid
  * Pre-installing shared libraries
  * Updating configuration
  * Finding an open port
  * Finding a unique name for a node name
- Tutorial uses buildx https://github.com/docker/buildx
  "buildx is a Docker CLI plugin for *extended build capabilities* with BuildKit."
*** Building Images
- Official Docker Images https://hub.docker.com/%5F/erlang/
  * They include rebar3 and come in alpine and debian flavors
  * Tags are updated based on new rebar3 and OS
- Mirror Docker images for this tutorial
  * docker.pkg.github.com/adoptingerlang/service_discovery/
  * us.gcr.io/adoptingerlang/
- Dockerfile reference https://github.com/moby/buildkit/blob/master/frontend/dockerfile/docs/reference.md
- It is recommended to add *.git* to *.dockerignore* to avoid
  invalidation on cache on "COPY . ."
- in recent Docker releases (18.06 and later) the abilities to
  mount secrets and SSH agent connections or keys in a secure manner.
  1) Private Hex dependencies: *secret* mount
     $ docker build --secret id=hex.config,src=~/.config/rebar3/hex.config .
   #+begin_src dockerfile
RUN --mount=type=secret,id=hex.config,target=/root/.config/rebar3/hex.config \
  rebar3 compile
   #+end_src
  2) Git dependencies: *ssh* mount
     $ export DOCKER_BUILDKIT=1
     $ docker build --ssh default .
     #+begin_src dockerfile
RUN apk add --no-cache openssh-client git && \
    mkdir -p -m 0600 ~/.ssh && \
    ssh-keyscan github.com >> ~/.ssh/known_hosts && \
    git config --global url."git@github.com:".insteadof "https://github.com/"
WORKDIR /src
COPY rebar.config rebar.lock .
RUN --mount=type=ssh \
  rebar3 compile
     #+end_src
  3) Docker 19.03, experimental *cache* mount
     #+begin_src dockerfile
WORKDIR /app/src
ENV REBAR_BASE_DIR /app/_build # without it would have been /app/src/_build
COPY rebar.config rebar.lock .
RUN --mount=type=cache,id=hex-cache,sharing=locked,target=/root/.cache/rebar3 \
  rebar3 compile
# - a mount of type=bind (the default), instead of a "COPY . ."
#   same result, but without creating a layer from copying files
# - immutable by default, aka it will fail if anything is copied to /app/src
#   which is why we set REBAR_BASE_DIR
RUN --mount=target=. \
    --mount=type=cache,id=hex-cache,sharing=locked,target=/root/.cache/rebar3 \
  rebar3 compile
   #+end_src
- Local vs remote cache ????
**** Multistage build
- $ docker buildx build -o type=docker \
  --target runner \
  --tag service_discovery:$(git rev-parse HEAED) .
- Can be tagged twice
  - with git ref (aka git rev-parse HEAD)
  - with the name of the branch (aka git symbolic-ref --short HEAD)
- A multi-stage Dockerfile can be used to copy the final release,
  from the stage it was built to a stage with Alpine base (and shared libs)
- Needed image
  - one with the built release,
    should NOT contain anything not required for running the release
    eg not: rebar3, git, Erlang/OTP
#+begin_src dockerfile
  # syntax = docker/dockerfile:experimental
  FROM docker.pkg.github.com/adoptingerlang/service_discovery/erlang:22.1.1-alpine as builder
  WORKDIR /app/src
  ENV REBAR_BASE_DIR /app_build
  # git for non-hex dependencies
  RUN --mount=type=cache,id=apk,sharing=locked,target=/var/cache/apk \
      apk add --update git
  # build and cache dependencis as their own layer
  COPY rebar.config rebar.lock .
  RUN --mount=type=cache,id=hex-cache,target=/root/.cache/rebar3 \
      rebar3 compile

  FROM builder as prod_compiled
  RUN --mount=type=bind,target=. \
      --mount=type=cache,id=hex-cache,target=/root/.cache/rebar3 \
      rebar3 as prod compile

  FROM prod_compiled as releaser
  WORKDIR /app/src
  RUN mkdir -p /opt/rel # directory to unpack the release to
  RUN --mount=type=cache,id=apk,sharing=locked,target=/var/cache/apk \
      apk add --update tar
  # tar and untar, to be copied in next stage
  # there are some changes made to the release when tarring
  # needed by tools like "releas_handler"
  # eg: RelName.boot is renamed start.boot.
  # https://www.erlang.org/doc/man/systools.html#make_tar-1
  RUN --mount=type=bind,target=. \
      --mount=type=cache,id=hex-cache,target=/root/.cache/rebar3 \
      rebar3 as prod tar && \
      tar zxvf $REBAR_BASE_DIR/prod/rel/*/*.tar.gz -C /opt/rel

  FROM docker.pkg.github.com/adoptingerlang/service_discovery/alpine:3.10.2 as runner
  WORKDIR /opt/service_discovery
  ENV COOKIE=service_discovery \
      # writes files generated by release script startup to /tmp
      # because /opt/service_discovery is owned by root and we cannot write there
      # eg: sys.config and vm.args, need to be generated from .src
      RELX_OUT_FILE_PATH=/tmp \
      # service_discovery specific env variables
      DB_HOST=127.0.0.1 \
      LOGGER_LEVEL=debug \
      SCHEDULERS=1
  EXPOSE 8053
  EXPOSE 3000
  EXPOSE 8081
  # openssl needed by the crypto app
  RUN --mount=type=cache,id=apk,sharing=locked,target=/var/cache/apk \
      ln -vs /var/cache/apk /etc/apk/cache && \
      apk add --update openssl ncurses
  COPY --from=releaser /opt/rel .
  ENTRYPOINT ["/opt/service_discovery/bin/service_discovery"]
  CMD ["foreground"]
#+end_src
- Prod profile on rebar.config
  - include_erts=true means the tarball contains the Erlang runtime,
    and can be run on a target that doesn't have erlang installed.
  #+begin_src
    {profiles, [{prod, [{relx, [{dev_mode, false},
			    {include_erts, true},
			    {include_src, false},
			    {debug_info, strip}]}]
	    }]}.
  #+end_src
*** Running a Container
*** Building and Publishing Images in CI
*** Next Steps
* Learn some erlang for greater good
- return {setup,...} from suffixed _test_() to lifecycle a test generator
- return {foreach,...} for doing individual lifecycles for the test generators
- spawn,timeout,inorder,inparallel are some other test generator options
** TODO 16 Event Handlers (gen_event)
- In the shape of a ~Event Manager~ or ~Event Forwarder~
  - No new process are spawned for short-lived tasks
  - Used when there is no need to wait in standby for new events
- Event handlers run in the same process as their managers
- Might be useful when there are many "listener" for the event
- the handler is a gen_event too, where the important part is the ~handle_event~
** 18 Building an Application
- Different Types of State:
  1) static
  2) recomputable dynamic
  3) uncomputable dynamic
- "The idea of an ~onion-layered system~ is to allow all of these _different states_
   to be protected correctly, by isolating different kinds of code from
   each other. In other words, it’s _process segregation._"
- The ~error kernel~ of your application is the place where your app is
  not allowed to fail, due non recomputable data loss.
  You use *try ..  catch* expressions here.
- Keep things on different supervision trees
** 19 Building Applications the OTP way
- Properties on .app.src file are optional.
  But some applications might use them...
- ~application_controller~ process starts with the erlang vm.
  It indirectly starts all other applications supervisers
    by starting an application master for each app.
- start/2
  start(Type, Args)
        Type: normal for local apps
        Args: What was put on the app file ({mod, {YourMod, Args}})
- stop/1
  stop(State)
  Clean up
- Application start/2 strategies
  temporary: abnormal ending, no restart
  transient: abnormal ending, no restart, stop all other apps, shutdown vm
  permanent: normal ending, stop all other apps shutdowns vm
             abnormal ending, ditto
** TODO 24 EUnited Nations Council
- -include_lib("eunit/include/eunit.hrl").
- Automatically:
   - Exports and runs functions ending with *_test()*
   - Looks for *_tests* module.
*** Macros ( ?assert separated for clarity)
| ?assert |           | (Exp)                |
| ?assert | Not       | (Exp)                |
| ?assert | Equal     | (A,B)                |
| ?assert | Match     | (Pattern, Exp)       |
| ?assert | NotMatch  | (Pattern, Exp)       |
| ?assert | Error     | (Pattern, Exp)       |
| ?assert | Throw     | (Pattern, Exp)       |
| ?assert | Exit      | (Pattern, Exp)       |
| ?assert | Exception | (Class,Pattern, Exp) |
*** Test Generators:
  - ~test generator functions~ that end with *_test_()*
  - ~test generator~ macros like *?_assertSomething*
  - It allows eunit:test() to select those tests generators with a
    ~test representation~
    - {module, Mod}
      {dir, Path}
      {file, Path}
      {generator, Fun}
      {application, AppName}
*** Fixtures
** 25 Bears, ETS, Beets: In-Memory NoSQL for Free!
- Use case: when you need to share data with more processes.
  And want to avoid the msg passing.
- Soft Limit of 1400 tables
- Store tuples, and only tuples.
  - 1 of the elements, will act as a primary key
- atoms starting with "$" are special like
  $end_of_table
- ETS has no transactions, unsafe operations are better handled by the owner itself
*** Types
  - set: unique primary key
  - ordered set: sorted and unique primary key. Useful when doing range ops
  - bag: non unique primary key, while the rest of the values are different
  - duplicate bag: non unique primary, and non different values
*** Modes
|           | Read   | Write  |
|-----------+--------+--------|
| public    | anyone | anyone |
| protected | anyone | owner  |
| private   | owner  | owner  |
*** New
- ets:new(Name, Opts) or ets:setopts()
  - named_table       : if the Name will refer the table later
  - {keypos, Position}: primary key position
  - {heir, Pid,
     Data} : ownership to Pid
  - {read_concurrency , true} : tune for burst write/reads, can be both enabled
    {write_concurrency, true}
  - compressed: compress non primary key fields
*** Insert/Del
- ets:delete(Table, Key)
- ets:insert(Table, Key, Values)
  ets:insert(Table, Key, [Values])
  on sets and ordered set replace key if provideda an existing key
- ets:insert_new()
  on set and ordered sets insert only if primary key is missing
*** Lookup
- ets:lookup(Table, Key) % Returns a List
  ets:lookup_element(Table, Key, Position)
- ets: first, next, last, prev
*** Match
- ~high order pattern-matching~ is NOT available on Erlang
  That is passing a pattern to a function to return the match.
- We do have however a special syntax for mach a ets query
  {atom, '$1', '_'}
- ets:match        % ret variables of the pattern
  ets:match_object % ret the whole entry
  ets:match_delete
*** (match) Select
- there is an extended query language to create
  "guard" like statements for the matchs
- ~parser transformer~ that can be used to transform
  a function to a matcher spec
- - At compile time, it replaces the function definition with the MS
  -include("stlib/include/ms_transform.hrl").
  ets:fun2ms(fun (X) -> when X > 4 -> X end).
- Requires, single variable OR tuple.
  Can destructure records.
- ets:select
  ets:select_reverse
  ets:select_count
  ets:select_delete
*** DETS
- no ordered_set support
- dets:open_file
  dets:close
** 26 Distribunomicon
- A computer can run up to 50 erlang VMs/Nodes
- Each node is named
- They connect to 1 EPMD (nameserver process)
*** 8 Fallacies of distributed computing:
| The *network* is reliable       | add redundancy, monitor down hosts                                                                     |
| There is NO latency             | timeout accordingly                                                                                    |
| Bandwith is infinite            | send messages about what happened, not what happened                                                   |
| The *network* is secure         | ssl distribution, or your own communication protocol                                                   |
| Topology does NOT change        | there are libraries that let us forget about nodenames                                                 |
| There is only one administrator | NO library can help                                                                                    |
| Transport cost is zero          | time(serialize/deserialize) and money (bandwith usage). Communication between nodes can be changed (?) |
| The *network* is homogeneous    | related to the Erlang node protocol. Can exists "C nodes". Or BERT-RPC                                 |
- For erlang: Unreachables nodes are dead nodes. Reachable nodes are alive.
*** CAP.
| Consistency         | same data across nodes                                               |
| Availability        | get a response for each request                                      |
| Partition Tolerance | parts of the "whole" can keep working while others can't communicate |
- CP: is about stopping modifications, to keep consistency.
      While keeping the partitions read-only operational.
- AP: will allow each partition to update. Resulting on inconsistent data.
- IRL quorum bases system can decide to modify data based on how many % nodes are alive.
  Trading off only some of the consistency in favor of availability.
*** Node names need to be UNIQUE
  - -sname (without dots)
  - -name (with dots)
- net_kernel:connect_node(NAME@HOST).
  net_adm:ping(NAME@HOST).
- node().
  nodes().
- {registeredid, NODE} ! {a,tuple,of,whatever}
- erlang:monitor_node(NODE, Bool) (link, and monitor still work across nodes)
- PID
  - 1st number where the node really comes from, 0 if it comes from the current node
  - 2nd a counter (?)
  - 3rd a second counter
- process_flag(trap_exit, true).
  link(OtherShell).
  erlang:monitor(process, OtherShell).
- spawn/2 spawn(NODE,function) aka a RPC
- net_kernel:start([foo, shortname])
  net_kernel:set_net_ticktime(5).
  net_kernel:stop().
*** Hidden Node
- erlang:send(Dest, Msg, [noconnect])
- erl -sname foo -hidden
  Will create all connections in the "hidden" pool
*** Firewall
- port 4369 for EPMD
- -kernel inet_dist_listen_min 9100
  -kernel inet_dist_listen_max 9115
- ports.conf
  [{kernel, [{inet_dist_listen_min, 9100},
             {inet_dist_listen_max, 9115}]}].
  erl -sname foo -config ports
*** Cookie
- A ~cookie~ is more akin to a username than a password.
  That allows to only nodes that know the cookie to comunicate to each other.
  Like dividing nodes in different cluster on the same hardware.
- -setcookie 'mYcOokie'
- erlang:set_cookie/2
- created automaticaly if not set, and stored in .erlang.cookie
*** Creating a remote shell, and connecting to it
local  > ^G
       > r remote@localhost
       > j 2
remote >
*** global:
- process registry that replicates data, handles node failure
- global:register_name/2
  global:unregister_name/1
  global:whereis_name/1
  global:send/2
- name conflict resolution
  - global:register_name/3
  - global:random_exit_name/3
    global:random_notify_name/3
    global:notify_all_name/3
*** rpc:
| rpc:call            | (Node,  Mod, Fun, Args)          |                 |
| rpc:call            | (Node,  Mod, Fun, Args, Timeout) |                 |
|---------------------+----------------------------------+-----------------|
| rpc:multicall       | (Nodes, Mod, Fun, Args)          | -- Call         |
| rpc:eval_everywhere | (Nodes, Mod, Fun, Args)          | -- Cast         |
| rpc:cast            | (Node,  Mod, Fun, Args)          |                 |
|---------------------+----------------------------------+-----------------|
| rpc:async_call      | (Node,  Mod, Fun, Args)          | returns a *Key* |
| rpc:yield           | (Key)                            |                 |
| rpc:nb_yield        | (Key, Timeout)                   |                 |
| rpc:nb_yield        | (Key)                            | -- Pooling      |
|---------------------+----------------------------------+-----------------|
** 27 Distributed OTP Application
- a single ~application controler~
    over many ~application masters~
      over supervisors of each app
- On a distributed application, a ~distributed application controller~ aka ~dist_ac~ is started (on the other nodes?)
  - An application can then be started or running. Whether is running on the node or waiting (started) for the node to die.
- failover: run the app in the case of a node dead
  takeover: force the app to run on your node again
- Coding an application:
  - The interface (single argument function) goes (?) on the application file
  - To make it distributed add start(normal. []) and start({takeover, _OtherNode}, []) to start_link()
    - Also add config/SNAME.config for each node and add the:
      - distributed
      - sync_nodes_mandatory
      - sync_nodes_timeout
** TODO 29 Mnesia and the Art of Remembering
- Mnesia is a layer built on top of ETS and DETS.
  DETS persistance and ETS performance.
  Automatically replicate data across nodes.
  Transactions support.
  For "small data" on a limited number of nodes.
- Mnesia tables
  - Have no built-in type constrains
  - Are global to all nodes in the cluster (add a prefix)
* Erlang and OTP in Action | Martin Logan
- include/ .hrl files part of your public API, private ones should be kept on src/
** 6 Implementing a Cache System
*** The design of the cache
|--------------+-------------+-------------------------------------------------|
| simple_cache | code        | user API, the application's face to the outside |
| sc_app       | application | the application behavior implementation         |
| sc_sup       | supervisor  | the root supervisor implementation              |
| sc_store     | process     | does the key - pid mapping, uses ETS            |
| sc_element   | gen_server  | that spawn to store each key of data            |
|--------------+-------------+-------------------------------------------------|
- Modules naming convention
  - use a common prefix for all modules of an application
    eg: sc_ for "simple cache"
  - except, for a main "user API" module, which uses the same name as the application
    eg: simple_cache
- sc_store   - while you keep a map of key->PID
  sc_element - you store different key values on different processes
*** Creating the basic OTP application skeleton
- applications
  - OTP needs metadata about the application in order to start or perform safe hot-code upgrades
  - name of the .app should match the name of the application
    - which is not necessarily the name of any module
  - {modules [M1,M2]}
    list the modules involved in this application
  - {registered [M1,M2]}
    list the modules that are properly registered
    eg: the root supervisor's name
  - {app, N}
    name of the application behaviour module
- =simple_one_for_one=
  - is limited to only 1(one) type of children
  - can start any number of children
  - dynamicallly added at runtime
  - no child is started when the supervisor starts up
  - restart   => temporaty,
  - shutdown  => brutal_kill
    tells what happens when the supervisor shutdown
- supervisor:start_child/2
  - second argument are given to the child's ~start_link/?~ with arity same as the numbers of arguments
    which are then passed to ~init/2~ by ~gen_server:start_link~
*** From application skeleton to a working cache
- Encapsulation
  - Users of the gen_server, don't need to know about (other modules?) the root supervisor.
    You create an wrapper API for it that calls the root supervisor function create_child
  - if a function returns a value or none, you can wrap it on {ok,} {error,}
    which is more palatable, and a more general shape that we can then use with different backend
- There is no registed name for any of the sc_element processes.
  This means the API functions MUST include the PID.
  It's the client problem to keep track of these identifiers.
- gen_server:start_link/3 blocks until init/1 returns
- ~timeout~, aka the thir value of the return tuple
  - is NOT passed as argument to the handler, so you need recompute it
  - in milliseconds
  - is passed around from init/1 to handle_call/handle_info
  - if you forget it, it will rever to _infinity_
- {stop, normal, State}
  return value of a handler to stop the gen_server process, "normal"lly without restaring it
- ~ETS~
  - in memory "hash-tables"
  - entries stored as _tuples_, where the first value is the key
  - may be _shared_ by a number of different processes *on* the VM
  - mainly flat, and preferably _without foreign key_ relationships with other table
  - can be a _named_table_ for easier access
  - ets:match - '_', '$1'
- Style: Where to put initialization triggering code? sc_store:init/1 call
  - sc_app
    - YES
    - on start/2
  - sc_sup
    - NO
    - it's a good design principle to avoid application code on supervisors.
    - Keep them small and reliable.
    - forgivable to put in init/1, because if it fails the application won't start
- try/catch
  - _Class:_Exception
  - useful when you have a sequence of things that must be done in order,
    and the result be the same if any of the steps fail
    eg: several actions trying matching {ok, _} OR catch it with and {error,}
    me: like a monad (?!) sorta
** TODO 7 Logging and Event Handling in Erlang
*** Logging in Erlang OTP
- SASL (System Architecture Support Libraries)
  - OTP SASL is unrelated to SASL library for authentication
- error_logger:
  - [warning,error,info]_msg/[1,2]
    - args are formatString and arguments
  - [warning,error,info]_report[1,2]
    - args are string OR *type* and string
- Custom *types* are ignored unless you define your own _event handler_
- SASL adds such handlers, which listen for reports send by OTP behaviours when supervisors start/stop/"dies"
  - aka provides "crash reports"
- the purpose of ~handle_info~ on gen_server is to handle _out-of-band messages_
- > application:start(sasl)
- erlang:spawn() will NOT give you the SASL report
  proc_lib:spawn() will give you the SASL report, spawns the OTP way
*** A custom event handler with gen_event
- replaces *handle_cast* with *handle_event*
- gen_event
  - container is often called a _event manager_
  - it initially has no callback module
    instead one or more handlers can be added (and removed again) dynamically after the container initialized
  - when an event is posted ALL currently _event handlers_ modules are called individually to handle the event
  - be mindful of how you handle the state in the _event handler_ because other handlers will touch it too
- gen_server
  - you tell it which callback module to use
    aka tied to a particular implementation (callback)
- 179
*** Adding a custom event stream to "simple cache"
