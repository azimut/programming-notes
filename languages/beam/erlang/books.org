- https://github.com/ericdouglas/distributed-systems-journey
- https://github.com/zkessin/testing-erlang-book
- https://github.com/inaka/erlang_guidelines
- https://adoptingerlang.org/
- https://github.com/oreillymedia/etudes-for-erlang
- https://github.com/heroku/erlang-in-anger
- beam https://github.com/happi/theBeamBook
- beam http://beam-wisdoms.clau.se/en/latest/
* Book: Adopting Erlang
https://adoptingerlang.org/
** Releases
- Starting an erlang release is similar to an OS boot sequence
- =boot file= an Erlang node starts by running instructions found in it
  http://erlang.org/doc/man/script.html
- .app (Application resource file)
- .rel (Release resource file)
  - modules to load and the dependencies of each
- .rel + .app(s)  of each app
  -> systools               -> .script
  -> systools:script2boot/1 -> .boot
- History of release tools:
  1) systools
  2) reltool (bundled with OTP)
  3) relx (bundled with rebar3)
- ~sasl~ application is required by ~rebar3 release~
  ONLY IF you need release upgrades or downgrades functionality
  IF working with container images, it's not needed to be included
- _build/default/rel/service_discovery/bin/service_discovery console
- rebar3 release
  rebar3 as prod tar
  rebar3 as prod release
*** relx
- {dev_mode,true}
  causes to use symlinks for apps directories for faster development loops when starting/stopping the release
- relx section on rebar3.config
  - {release,{NAME,VERSION},APPLICATIONS}
    - VERSION can be {git,long}
    - APPLICATIONS order is important
*** Building a development release
*** Building a production release
- {profiles, [{prod, [{relx, []}]}]}
  - override:
    | dev_mode       | false       | to copy NOT symlink _build/             |
    | include_erts   | true        | to include erlang runtime               |
    | include_src    | false       | save space                              |
    | debug_info     | strip       | save space, used by debugger,xref,cover |
    | sys_config_src | path string |                                         |
    | vm_args_src    | path string |                                         |
*** Runtime configuration
*** Coming up
* Book: Learn some erlang for greater good
- return {setup,...} from suffixed _test_() to lifecycle a test generator
- return {foreach,...} for doing individual lifecycles for the test generators
- spawn,timeout,inorder,inparallel are some other test generator options
** TODO 16 Event Handlers (gen_event)
- In the shape of a ~Event Manager~ or ~Event Forwarder~
  - No new process are spawned for short-lived tasks
  - Used when there is no need to wait in standby for new events
- Event handlers run in the same process as their managers
- Might be useful when there are many "listener" for the event
- the handler is a gen_event too, where the important part is the ~handle_event~
** 18 Building an Application
- Different Types of State:
  1) static
  2) recomputable dynamic
  3) uncomputable dynamic
- "The idea of an ~onion-layered system~ is to allow all of these _different states_
   to be protected correctly, by isolating different kinds of code from
   each other. In other words, itâ€™s _process segregation._"
- The ~error kernel~ of your application is the place where your app is
  not allowed to fail, due non recomputable data loss.
  You use *try ..  catch* expressions here.
- Keep things on different supervision trees
** 19 Building Applications the OTP way
- Properties on .app.src file are optional.
  But some applications might use them...
- ~application_controller~ process starts with the erlang vm.
  It indirectly starts all other applications supervisers
    by starting an application master for each app.
- start/2
  start(Type, Args)
        Type: normal for local apps
        Args: What was put on the app file ({mod, {YourMod, Args}})
- stop/1
  stop(State)
  Clean up
- Application start/2 strategies
  temporary: abnormal ending, no restart
  transient: abnormal ending, no restart, stop all other apps, shutdown vm
  permanent: normal ending, stop all other apps shutdowns vm
             abnormal ending, ditto
** TODO 24 EUnited Nations Council
- -include_lib("eunit/include/eunit.hrl").
- Automatically:
   - Exports and runs functions ending with *_test()*
   - Looks for *_tests* module.
*** Macros
  ?assert(Exp), ?assertNot(Exp)
  ?assertEqual(A,B)
  ?assertMatch(Pattern, Exp), ?assertNotMatch(Pattern, Exp)
  ?assertError(Pattern, Exp)
  ?assertThrow(Pattern, Exp)
  ?assertExit(Pattern, Exp)
  ?assertException(Class,Pattern, Exp)
*** Test Generators:
  - ~test generator functions~ that end with *_test_()*
  - ~test generator~ macros like *?_assertSomething*
  - It allows eunit:test() to select those tests generators with a
    ~test representation~
    - {module, Mod}
      {dir, Path}
      {file, Path}
      {generator, Fun}
      {application, AppName}
*** Fixtures
** 25 Bears, ETS, Beets: In-Memory NoSQL for Free!
- Use case: when you need to share data with more processes.
  And want to avoid the msg passing.
- Soft Limit of 1400 tables
- Store tuples, and only tuples.
  - 1 of the elements, will act as a primary key
- atoms starting with "$" are special like
  $end_of_table
- ETS has no transactions, unsafe operations are better handled by the owner itself
*** Types
  - set: unique primary key
  - ordered set: sorted and unique primary key. Useful when doing range ops
  - bag: non unique primary key, while the rest of the values are different
  - duplicate bag: non unique primary, and non different values
*** Modes
|           | Read   | Write  |
|-----------+--------+--------|
| public    | anyone | anyone |
| protected | anyone | owner  |
| private   | owner  | owner  |
*** New
- ets:new(Name, Opts) or ets:setopts()
  - named_table       : if the Name will refer the table later
  - {keypos, Position}: primary key position
  - {heir, Pid,
     Data} : ownership to Pid
  - {read_concurrency , true} : tune for burst write/reads, can be both enabled
    {write_concurrency, true}
  - compressed: compress non primary key fields
*** Insert/Del
- ets:delete(Table, Key)
- ets:insert(Table, Key, Values)
  ets:insert(Table, Key, [Values])
  on sets and ordered set replace key if provideda an existing key
- ets:insert_new()
  on set and ordered sets insert only if primary key is missing
*** Lookup
- ets:lookup(Table, Key) % Returns a List
  ets:lookup_element(Table, Key, Position)
- ets: first, next, last, prev
*** Match
- ~high order pattern-matching~ is NOT available on Erlang
  That is passing a pattern to a function to return the match.
- We do have however a special syntax for mach a ets query
  {atom, '$1', '_'}
- ets:match        % ret variables of the pattern
  ets:match_object % ret the whole entry
  ets:match_delete
*** (match) Select
- there is an extended query language to create
  "guard" like statements for the matchs
- ~parser transformer~ that can be used to transform
  a function to a matcher spec
- - At compile time, it replaces the function definition with the MS
  -include("stlib/include/ms_transform.hrl").
  ets:fun2ms(fun (X) -> when X > 4 -> X end).
- Requires, single variable OR tuple.
  Can destructure records.
- ets:select
  ets:select_reverse
  ets:select_count
  ets:select_delete
*** DETS
- no ordered_set support
- dets:open_file
  dets:close
** 26 Distribunomicon
- A computer can run up to 50 erlang VMs/Nodes
- Each node is named
- They connect to 1 EPMD (nameserver process)
*** 8 Fallacies of distributed computing:
| The *network* is reliable       | add redundancy, monitor down hosts                                                                     |
| There is NO latency             | timeout accordingly                                                                                    |
| Bandwith is infinite            | send messages about what happened, not what happened                                                   |
| The *network* is secure         | ssl distribution, or your own communication protocol                                                   |
| Topology does NOT change        | there are libraries that let us forget about nodenames                                                 |
| There is only one administrator | NO library can help                                                                                    |
| Transport cost is zero          | time(serialize/deserialize) and money (bandwith usage). Communication between nodes can be changed (?) |
| The *network* is homogeneous    | related to the Erlang node protocol. Can exists "C nodes". Or BERT-RPC                                 |
- For erlang: Unreachables nodes are dead nodes. Reachable nodes are alive.
*** CAP.
| Consistency         | same data across nodes                                               |
| Availability        | get a response for each request                                      |
| Partition Tolerance | parts of the "whole" can keep working while others can't communicate |
- CP: is about stopping modifications, to keep consistency.
      While keeping the partitions read-only operational.
- AP: will allow each partition to update. Resulting on inconsistent data.
- IRL quorum bases system can decide to modify data based on how many % nodes are alive.
  Trading off only some of the consistency in favor of availability.
*** Node names need to be UNIQUE
  - -sname (without dots)
  - -name (with dots)
- net_kernel:connect_node(NAME@HOST).
  net_adm:ping(NAME@HOST).
- node().
  nodes().
- {registeredid, NODE} ! {a,tuple,of,whatever}
- erlang:monitor_node(NODE, Bool) (link, and monitor still work across nodes)
- PID
  - 1st number where the node really comes from, 0 if it comes from the current node
  - 2nd a counter (?)
  - 3rd a second counter
- process_flag(trap_exit, true).
  link(OtherShell).
  erlang:monitor(process, OtherShell).
- spawn/2 spawn(NODE,function) aka a RPC
- net_kernel:start([foo, shortname])
  net_kernel:set_net_ticktime(5).
  net_kernel:stop().
*** Hidden Node
- erlang:send(Dest, Msg, [noconnect])
- erl -sname foo -hidden
  Will create all connections in the "hidden" pool
*** Firewall
- port 4369 for EPMD
- -kernel inet_dist_listen_min 9100
  -kernel inet_dist_listen_max 9115
- ports.conf
  [{kernel, [{inet_dist_listen_min, 9100},
             {inet_dist_listen_max, 9115}]}].
  erl -sname foo -config ports
*** Cookie
- A ~cookie~ is more akin to a username than a password.
  That allows to only nodes that know the cookie to comunicate to each other.
  Like dividing nodes in different cluster on the same hardware.
- -setcookie 'mYcOokie'
- erlang:set_cookie/2
- created automaticaly if not set, and stored in .erlang.cookie
*** Creating a remote shell, and connecting to it
local  > ^G
       > r remote@localhost
       > j 2
remote >
*** global:
- process registry that replicates data, handles node failure
- global:register_name/2
  global:unregister_name/1
  global:whereis_name/1
  global:send/2
- name conflict resolution
  - global:register_name/3
  - global:random_exit_name/3
    global:random_notify_name/3
    global:notify_all_name/3
*** rpc:
| rpc:call            | (Node,  Mod, Fun, Args)          |                 |
| rpc:call            | (Node,  Mod, Fun, Args, Timeout) |                 |
|---------------------+----------------------------------+-----------------|
| rpc:multicall       | (Nodes, Mod, Fun, Args)          | -- Call         |
| rpc:eval_everywhere | (Nodes, Mod, Fun, Args)          | -- Cast         |
| rpc:cast            | (Node,  Mod, Fun, Args)          |                 |
|---------------------+----------------------------------+-----------------|
| rpc:async_call      | (Node,  Mod, Fun, Args)          | returns a *Key* |
| rpc:yield           | (Key)                            |                 |
| rpc:nb_yield        | (Key, Timeout)                   |                 |
| rpc:nb_yield        | (Key)                            | -- Pooling      |
|---------------------+----------------------------------+-----------------|
** 27 Distributed OTP Application
- a single ~application controler~
    over many ~application masters~
      over supervisors of each app
- On a distributed application, a ~distributed application controller~ aka ~dist_ac~ is started (on the other nodes?)
  - An application can then be started or running. Whether is running on the node or waiting (started) for the node to die.
- failover: run the app in the case of a node dead
  takeover: force the app to run on your node again
- Coding an application:
  - The interface (single argument function) goes (?) on the application file
  - To make it distributed add start(normal. []) and start({takeover, _OtherNode}, []) to start_link()
    - Also add config/SNAME.config for each node and add the:
      - distributed
      - sync_nodes_mandatory
      - sync_nodes_timeout
** TODO 29 Mnesia and the Art of Remembering
- Mnesia is a layer built on top of ETS and DETS.
  DETS persistance and ETS performance.
  Automatically replicate data across nodes.
  Transactions support.
  For "small data" on a limited number of nodes.
- Mnesia tables
  - Have no built-in type constrains
  - Are global to all nodes in the cluster (add a prefix)
* Book: Erlang and OTP in Action | Martin Logan
- include/ .hrl files part of your public API, private ones should be kept on src/
** 6 Implementing a Cache System
*** The design of the cache
|--------------+-------------+-------------------------------------------------|
| simple_cache | code        | user API, the application's face to the outside |
| sc_app       | application | the application behavior implementation         |
| sc_sup       | supervisor  | the root supervisor implementation              |
| sc_store     | process     | does the key - pid mapping, uses ETS            |
| sc_element   | gen_server  | that spawn to store each key of data            |
|--------------+-------------+-------------------------------------------------|
- Modules naming convention
  - use a common prefix for all modules of an application
    eg: sc_ for "simple cache"
  - except, for a main "user API" module, which uses the same name as the application
    eg: simple_cache
- sc_store   - while you keep a map of key->PID
  sc_element - you store different key values on different processes
*** Creating the basic OTP application skeleton
- applications
  - OTP needs metadata about the application in order to start or perform safe hot-code upgrades
  - name of the .app should match the name of the application
    - which is not necessarily the name of any module
  - {modules [M1,M2]}
    list the modules involved in this application
  - {registered [M1,M2]}
    list the modules that are properly registered
    eg: the root supervisor's name
  - {app, N}
    name of the application behaviour module
- =simple_one_for_one=
  - is limited to only 1(one) type of children
  - can start any number of children
  - dynamicallly added at runtime
  - no child is started when the supervisor starts up
  - restart   => temporaty,
  - shutdown  => brutal_kill
    tells what happens when the supervisor shutdown
- supervisor:start_child/2
  - second argument are given to the child's ~start_link/?~ with arity same as the numbers of arguments
    which are then passed to ~init/2~ by ~gen_server:start_link~
*** From application skeleton to a working cache
- Encapsulation
  - Users of the gen_server, don't need to know about (other modules?) the root supervisor.
    You create an wrapper API for it that calls the root supervisor function create_child
  - if a function returns a value or none, you can wrap it on {ok,} {error,}
    which is more palatable, and a more general shape that we can then use with different backend
- There is no registed name for any of the sc_element processes.
  This means the API functions MUST include the PID.
  It's the client problem to keep track of these identifiers.
- gen_server:start_link/3 blocks until init/1 returns
- ~timeout~, aka the thir value of the return tuple
  - is NOT passed as argument to the handler, so you need recompute it
  - in milliseconds
  - is passed around from init/1 to handle_call/handle_info
  - if you forget it, it will rever to _infinity_
- {stop, normal, State}
  return value of a handler to stop the gen_server process, "normal"lly without restaring it
- ~ETS~
  - in memory "hash-tables"
  - entries stored as _tuples_, where the first value is the key
  - may be _shared_ by a number of different processes *on* the VM
  - mainly flat, and preferably _without foreign key_ relationships with other table
  - can be a _named_table_ for easier access
  - ets:match - '_', '$1'
- Style: Where to put initialization triggering code? sc_store:init/1 call
  - sc_app
    - YES
    - on start/2
  - sc_sup
    - NO
    - it's a good design principle to avoid application code on supervisors.
    - Keep them small and reliable.
    - forgivable to put in init/1, because if it fails the application won't start
- try/catch
  - _Class:_Exception
  - useful when you have a sequence of things that must be done in order,
    and the result be the same if any of the steps fail
    eg: several actions trying matching {ok, _} OR catch it with and {error,}
    me: like a monad (?!) sorta
** TODO 7 Logging and Event Handling in Erlang
*** Logging in Erlang OTP
- SASL (System Architecture Support Libraries)
  - OTP SASL is unrelated to SASL library for authentication
- error_logger:
  - [warning,error,info]_msg/[1,2]
    - args are formatString and arguments
  - [warning,error,info]_report[1,2]
    - args are string OR *type* and string
- Custom *types* are ignored unless you define your own _event handler_
- SASL adds such handlers, which listen for reports send by OTP behaviours when supervisors start/stop/"dies"
  - aka provides "crash reports"
- the purpose of ~handle_info~ on gen_server is to handle _out-of-band messages_
- > application:start(sasl)
- erlang:spawn() will NOT give you the SASL report
  proc_lib:spawn() will give you the SASL report, spawns the OTP way
*** A custom event handler with gen_event
- replaces *handle_cast* with *handle_event*
- gen_event
  - container is often called a _event manager_
  - it initially has no callback module
    instead one or more handlers can be added (and removed again) dynamically after the container initialized
  - when an event is posted ALL currently _event handlers_ modules are called individually to handle the event
  - be mindful of how you handle the state in the _event handler_ because other handlers will touch it too
- gen_server
  - you tell it which callback module to use
    aka tied to a particular implementation (callback)
- 179
*** Adding a custom event stream to "simple cache"
